import { BaseChatMemory, ChatMessageHistory, InputValues, OutputValues, MemoryVariables } from "langchain/memory";
import { HumanMessage, AIMessage, SystemMessage, BaseMessage } from "@langchain/core/messages";
import { ConversationSummaryBufferMemory } from "langchain/memory";
import prisma from "@/lib/prisma";
import { ChatOpenAI } from "@langchain/openai";
import { MessageUserType } from "@prisma/client";

interface ConversationMemoryInput {
  llm: ChatOpenAI; // LLM to use for summarization
  humanPrefix?: string;
  aiPrefix?: string;
  memoryKey?: string;
  conversationId: string; // The ID of the current conversation
  maxTokenLimit?: number; // Max tokens for the combined history and summary
  sessionId?: string;
}

export class PrismaChatHistory extends ChatMessageHistory {
  private conversationId: string;

  constructor(conversationId: string) {
    super();
    this.conversationId = conversationId;
  }

  // Override the getMessages method to fetch from Prisma
  async getMessages(): Promise<BaseMessage[]> {
    const messages = await prisma.assistantMessage.findMany({
      where: { AND: [{ conversationId: this.conversationId }] },
      orderBy: { createdAt: "asc" },
    });

    return messages.map((msg) => {
      if (msg.role === "USER") {
        return new HumanMessage(msg.content || "");
      } else {
        return new AIMessage(msg.content || "");
      }
    });
  }

  // Override the addMessage method to save to Prisma
  async addMessage(message: BaseMessage): Promise<void> {
    await prisma.assistantMessage.create({
      data: {
        conversationId: this.conversationId,
        role: message.getType() == "human" ? "USER" : "SYSTEM",
        content: message.content.toString(),
      },
    });
    // Call the original addMessage to keep the in-memory buffer consistent if needed,
    // though for this specific use case, getMessages will always fetch from DB.
    // super.addMessage(message);
  }

  // You might also want to override clear if you need to delete messages from DB
  async clear(): Promise<void> {
    await prisma.assistantConversation.deleteMany({
      where: { id: this.conversationId },
    });
    // super.clear();
  }
}

export class PrismaConversationSummaryBufferMemory extends ConversationSummaryBufferMemory {
  private conversationId: string;
  private sessionId?: string;

  constructor(fields: ConversationMemoryInput) {
    const chatHistory = new PrismaChatHistory(fields.conversationId);

    super({
      llm: fields.llm,
      chatHistory: chatHistory,
      memoryKey: "history",
      humanPrefix: fields.humanPrefix,
      aiPrefix: fields.aiPrefix,
      maxTokenLimit: fields.maxTokenLimit,
      inputKey: "input",
      // You can also add a `currentSummary` field if you want to explicitly
      // load and save the summary from the Conversation model.
      // For simplicity, we'll let the LangChain internal logic manage the summary string.
    });

    this.conversationId = fields.conversationId;
    this.sessionId = fields.sessionId;
    this.returnMessages = true; // Always return messages for easy integration
  }

  // Override _loadMemoryVariables to fetch the summary from the database if it exists
  async loadMemoryVariables(values: InputValues): Promise<MemoryVariables> {
    const conversation = await prisma.assistantConversation.findUnique({
      where: { id: this.conversationId },
      select: { summary: true },
    });

    if (conversation?.summary) {
      // Set the internal summary for the ConversationSummaryBufferMemory
      // This is a bit of a hack as `currentSummary` is protected, but necessary
      // to rehydrate the memory correctly.
      (this as any).currentSummary = conversation.summary;
    }

    // Call the parent method to get buffered messages and potentially generate a new summary
    const memoryVariables = await super.loadMemoryVariables();

    // If a new summary was generated by the parent, update it in the database
    if (this.movingSummaryBuffer !== conversation?.summary) {
      await prisma.assistantConversation.update({
        where: { id: this.conversationId },
        data: { summary: this.movingSummaryBuffer },
      });
    }

    return memoryVariables;
  }

  // Override _saveContext to ensure the summary is saved after each turn
  async saveContext(inputValues: InputValues, outputValues: OutputValues): Promise<void> {
    await super.saveContext(inputValues, outputValues);

    // After the super method saves messages and potentially updates the summary,
    // we save the new summary to the database.
    await prisma.assistantConversation.update({
      where: { id: this.conversationId },
      data: { summary: this.movingSummaryBuffer },
    });
  }

  // Custom static method to create a new conversation and memory instance
  static async createMemory(
    llm: ChatOpenAI,
    maxTokenLimit: number,
    tenantId: string,
    agentId: string,
    sessionId: string,
    isUserSession: boolean
  ): Promise<PrismaConversationSummaryBufferMemory> {
    const newConversation = await prisma.assistantConversation.create({
      data: { tenantId, sessionId, isUserSession, agentId }, // Create an empty conversation
    });

    return new PrismaConversationSummaryBufferMemory({
      llm,
      conversationId: newConversation.id,
      maxTokenLimit,
    });
  }

  // Custom static method to load an existing conversation and memory instance
  static async loadMemory(
    llm: ChatOpenAI,
    conversationId: string,
    maxTokenLimit: number,
    sessionId: string
  ): Promise<PrismaConversationSummaryBufferMemory | null> {
    const conversation = await prisma.assistantConversation.findUnique({
      where: { id: conversationId },
    });

    if (!conversation) {
      return null;
    }

    return new PrismaConversationSummaryBufferMemory({
      llm,
      conversationId,
      maxTokenLimit,
      sessionId,
    });
  }

  // Helper to get the conversation ID
  getConversationId(): string {
    return this.conversationId;
  }

  getSessionId(): string | undefined {
    return this.sessionId;
  }
}
